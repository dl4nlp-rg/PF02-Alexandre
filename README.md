# PT-EN-Translator

This repository brings an implementation of T5 for translation in PT-EN and EN-PT tasks. We propose some changes in tokenizator and post-processing that improves the result. We have here also a report about our findings as well as the weights of the trained models. The training process for T5 in portuguese works like Figure 1. 

<img src="https://github.com/dl4nlp-rg/PF02-Alexandre/blob/master/figs/t5.png" width="500">
Figure 1: T5 training strategy with adition of red conections.  The purple boxeswere the fine-tuning part performed by this work.  Created using figure from [1]

For this project, we used https://paracrawl.eu/ corpus. We trained using 5M+ data from ParaCrawl. We just did not use more data because it was taking too much time. In theory, it would improve the results.

Another important contribution of this repository is the creation of two small corpus of paracrawl containing the Google Translate En - PT-pt and Google Translate PT-pt - En translations of 128000 sentences. This costed around $300 each to train, so you can save some money if you want to compare your results with Google Translate (GT).

## Instalation and Usage

To install it, just run 'pip install -r requirements.txt'

It you want to train it, make sure you have a 8GB GPU or change the batch sizes and train it again. On CPU it should work well for evaluating, but it must be time consuming for training. I did not try it.

## Organization

Here we have code for reproducing our results (training and testing) and for using it. Modules and notebooks are available for testing.

| Component | Description |
| ------ | ------ |
| [pt_en_translator](https://github.com/alelopes/PT-EN-Translator/) | Contains all codes for running the training, testing and just using the module to your own projects.
| [para_gt_dataset](https://github.com/alelopes/PT-EN-Translator/) | Contains pickle of Google Translate results for a subset of ParaCrawl.|
| [models](https://github.com/alelopes/PT-EN-Translator/) | Contains the link for the models of translation generated by our team. |
| [reports](https://github.com/alelopes/PT-EN-Translator/) | Contains the pdf of any report or publication created from this project. |
| [notebooks](https://github.com/alelopes/PT-EN-Translator/) | Examples on how to generate and use our project. |

## Training on your own data

If you are going to train using our model or use our testing corpora, It is important to deduplicate your data with testing set provided by us. Also, we think it is suitable to run MinHash LSH algorithm to discard any sentence with > 0.7 Jaccard Similarity with any testing word. We have code for this in para_gt_dataset and an example in the Notebooks.

## Results

Here we are going to compare the results of our algoritm with GT with the translated subset of ParaCrawl and in WMT19' Biomedical Pt-En and En-Pt tasks, comparing our results with the winning algorithm and MarianMT Implementation from https://huggingface.co/transformers/model_doc/marian.html. 

The results comparing with GT in the subset of ParaCrawl are available in Table 1. The results comparing with WMT19' Test Set is available in Table 2. It is important to notice that we obtain SOTA results in PT-EN translation in WMT19'. It is also important to notice that our project was trained in Pt-pt and as WMT19' is Pt-br, the results will possible increase if trained in a Pt-br corpus (both direction of translation).

Table 1: Results in ParaCrawl subset with sacreBLEU score

![alt text](https://github.com/dl4nlp-rg/PF02-Alexandre/blob/master/figs/results_gt.png)


Table 2: Results in WMT19' test set with sacreBLEU score

![alt text](https://github.com/dl4nlp-rg/PF02-Alexandre/blob/master/figs/results.png)

About the quality of the data for training, we understand that there is a lot to improve. But to get a sense of the translator's quality, you're reading a text excerpt translated by him. The translator is able to generate translations in a more complex context with difficult words. All this last paragraph is composed of text automatically translated by our project.

## Future Work

We are going to participate in the WMT'20 Biomedical challenge. Any findings in the participation will be compiled here and all reports will be shared in this repo as well. We believe that the same strategy could be improved if using a PT-br corpus. For the WMT'20 biomedical challenge it would be interesting to train in a larger corpus than what you did.

If you want to contribute to this work or team up with me for some other task (or other challenge in Translation), let me know!

## References

[1] Colin  Raffel,   Noam  Shazeer,   Adam  Roberts,   Katherine  Lee,   SharanNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.  Exploring the limits of transfer learning with a unified text-to-text transformer.arXivpreprint arXiv:1910.10683, 2019
